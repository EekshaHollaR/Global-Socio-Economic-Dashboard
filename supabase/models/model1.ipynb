{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ4O8MYcxyL_"
   },
   "source": [
    "new code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1760446416812,
     "user": {
      "displayName": "Swaroop Tandle",
      "userId": "04403577212311715821"
     },
     "user_tz": -330
    },
    "id": "sSEhZ4dmx1Ay",
    "outputId": "84157c0b-73fb-4733-8e0a-c8b0a2cf25ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country Name  Date  GDP growth (annual %)  \\\n",
      "0       Albania  2000               7.462859   \n",
      "1       Albania  2001               8.863731   \n",
      "2       Albania  2002               4.628396   \n",
      "3       Albania  2003               5.333264   \n",
      "4       Albania  2004               5.266262   \n",
      "5       Albania  2005               5.130822   \n",
      "6       Albania  2006               6.018981   \n",
      "7       Albania  2007               6.500093   \n",
      "8       Albania  2008               6.907062   \n",
      "9       Albania  2009               2.690752   \n",
      "10      Albania  2010               2.973154   \n",
      "11      Albania  2011               2.463517   \n",
      "12      Albania  2012               0.984130   \n",
      "13      Albania  2013               1.707228   \n",
      "14      Albania  2014               2.240227   \n",
      "15      Albania  2015               2.227704   \n",
      "16      Albania  2016               3.908936   \n",
      "17      Albania  2017               3.283176   \n",
      "18      Albania  2018               3.671419   \n",
      "19      Albania  2019               2.062578   \n",
      "\n",
      "    Inflation, consumer prices (annual %)  \\\n",
      "0                                0.050018   \n",
      "1                                3.107588   \n",
      "2                                7.770526   \n",
      "3                                0.484003   \n",
      "4                                2.280019   \n",
      "5                                2.366582   \n",
      "6                                2.370728   \n",
      "7                                2.932682   \n",
      "8                                3.320871   \n",
      "9                                2.266922   \n",
      "10                               3.626047   \n",
      "11                               3.429123   \n",
      "12                               2.031593   \n",
      "13                               1.937621   \n",
      "14                               1.625865   \n",
      "15                               1.896174   \n",
      "16                               1.275432   \n",
      "17                               1.986661   \n",
      "18                               2.028060   \n",
      "19                               1.411091   \n",
      "\n",
      "    Unemployment, total (% of total labor force) (modeled ILO estimate)  \\\n",
      "0                                              19.023                     \n",
      "1                                              18.570                     \n",
      "2                                              17.891                     \n",
      "3                                              16.985                     \n",
      "4                                              16.306                     \n",
      "5                                              15.966                     \n",
      "6                                              15.626                     \n",
      "7                                              15.966                     \n",
      "8                                              13.060                     \n",
      "9                                              13.674                     \n",
      "10                                             14.086                     \n",
      "11                                             13.481                     \n",
      "12                                             13.376                     \n",
      "13                                             15.866                     \n",
      "14                                             18.055                     \n",
      "15                                             17.193                     \n",
      "16                                             15.418                     \n",
      "17                                             13.616                     \n",
      "18                                             12.304                     \n",
      "19                                             11.466                     \n",
      "\n",
      "    Crisis_Score  Crisis_Flag Crisis_Level  \n",
      "0              2            1     Moderate  \n",
      "1              2            1     Moderate  \n",
      "2              2            1     Moderate  \n",
      "3              2            1     Moderate  \n",
      "4              2            1     Moderate  \n",
      "5              2            1     Moderate  \n",
      "6              2            1     Moderate  \n",
      "7              2            1     Moderate  \n",
      "8              2            1     Moderate  \n",
      "9              2            1     Moderate  \n",
      "10             2            1     Moderate  \n",
      "11             2            1     Moderate  \n",
      "12             1            0          Low  \n",
      "13             1            0          Low  \n",
      "14             1            0          Low  \n",
      "15             1            0          Low  \n",
      "16             1            0          Low  \n",
      "17             1            0          Low  \n",
      "18             1            0          Low  \n",
      "19             1            0          Low  \n",
      "\n",
      "ðŸ“‚ File saved as: economic_dataset_crisis_label.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (update the path if needed)\n",
    "data = pd.read_csv(r\"C:\\Users\\Eeksha\\Desktop\\GSECD-final-project1.0\\version2\\civic-data-forge-main\\supabase\\data\\dataset1.csv\")\n",
    "\n",
    "# --- Column names (copy these exactly from your file) ---\n",
    "gdp_col = \"GDP growth (annual %)\"\n",
    "infl_col = \"Inflation, consumer prices (annual %)\"\n",
    "unemp_col = \"Unemployment, total (% of total labor force) (modeled ILO estimate)\"\n",
    "credit_col = \"Domestic credit to private sector (% of GDP)\"\n",
    "exp_col = \"Exports of goods and services (% of GDP)\"\n",
    "imp_col = \"Imports of goods and services (% of GDP)\"\n",
    "\n",
    "# --- Ensure numeric (coerce non-numeric to NaN) ---\n",
    "for col in [gdp_col, infl_col, unemp_col, credit_col, exp_col, imp_col]:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "\n",
    "# --- Handle missing values ---\n",
    "# Option A (recommended): keep NaN and treat missing as not triggering the flag.\n",
    "# Option B: fill with reasonable values, e.g., forward-fill or median. Uncomment if desired.\n",
    "# data.fillna(method=\"ffill\", inplace=True)    # example forward-fill\n",
    "# data.fillna(data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# --- Compute crisis score (count of triggered conditions) ---\n",
    "data[\"Crisis_Score\"] = (\n",
    "    (data[gdp_col] < 0).astype(int) +                             # contraction\n",
    "    (data[infl_col] > 20).astype(int) +                           # serious inflation\n",
    "    (data[unemp_col] > 10).astype(int) +                          # high unemployment\n",
    "    (data[credit_col] > 100).astype(int) +                        # excessive private credit\n",
    "    (data[exp_col] < 15).astype(int) +                            # weak exports\n",
    "    ((data[imp_col] - data[exp_col]) > 20).astype(int)            # large trade gap\n",
    ")\n",
    "\n",
    "# --- Create binary Crisis_Flag: 1 if >=2 triggers, else 0 ---\n",
    "data[\"Crisis_Flag\"] = (data[\"Crisis_Score\"] >= 2).astype(int)\n",
    "\n",
    "# Optional: categorize severity\n",
    "def categorize(score):\n",
    "    if score >= 4:\n",
    "        return \"Severe\"\n",
    "    elif score >= 2:\n",
    "        return \"Moderate\"\n",
    "    elif score == 1:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "data[\"Crisis_Level\"] = data[\"Crisis_Score\"].apply(categorize)\n",
    "\n",
    "# Preview\n",
    "cols_to_show = [\"Country Name\", \"Date\", gdp_col, infl_col, unemp_col, \"Crisis_Score\", \"Crisis_Flag\", \"Crisis_Level\"]\n",
    "print(data[cols_to_show].head(20))\n",
    "\n",
    "# Save to csv\n",
    "out_fn = \"economic_dataset_crisis_label.csv\"\n",
    "data.to_csv(out_fn, index=False)\n",
    "print(f\"\\nðŸ“‚ File saved as: {out_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150348,
     "status": "ok",
     "timestamp": 1760446646076,
     "user": {
      "displayName": "Swaroop Tandle",
      "userId": "04403577212311715821"
     },
     "user_tz": -330
    },
    "id": "2RB9wsj3xvCQ",
    "outputId": "f1291708-d8e8-4f4f-fe5e-8cf810eef1f3"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import shap\n",
    "# import warnings\n",
    "\n",
    "# # -------------------------\n",
    "# # Config\n",
    "# # -------------------------\n",
    "# INPUT_CSV = \"/content/economic_dataset_crisis_label.csv\"\n",
    "# OUT_PRED_CSV = \"future_crisis_predictions.csv\"\n",
    "# RANDOM_STATE = 42\n",
    "# TEST_SIZE = 0.2\n",
    "# PROB_THRESHOLD = 0.90\n",
    "# N_ESTIMATORS = 200\n",
    "\n",
    "# # -------------------------\n",
    "# # 1) Load dataset\n",
    "# # -------------------------\n",
    "# data = pd.read_csv(INPUT_CSV)\n",
    "# data[\"Date\"] = data[\"Date\"].astype(int)\n",
    "\n",
    "# # Create numeric feature list (exclude target/metadata)\n",
    "# exclude_cols = [\"Country Name\", \"Country Code\", \"Date\", \"Crisis_Flag\", \"Crisis_Level\", \"Crisis_Score\"]\n",
    "# numeric_features = data.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# # -------------------------\n",
    "# # 2) Lag features (use last-year indicators)\n",
    "# # -------------------------\n",
    "# data = data.sort_values([\"Country Name\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# for col in numeric_features:\n",
    "#     data[f\"{col}_lag1\"] = data.groupby(\"Country Name\")[col].shift(1)\n",
    "\n",
    "# # Target: next-year crisis\n",
    "# data[\"Crisis_NextYear\"] = data.groupby(\"Country Name\")[\"Crisis_Flag\"].shift(-1)\n",
    "\n",
    "# # Drop rows with missing lag or target\n",
    "# lagged_cols = [f\"{c}_lag1\" for c in numeric_features]\n",
    "# data_clean = data.dropna(subset=lagged_cols + [\"Crisis_NextYear\"]).copy()\n",
    "# data_clean[\"Crisis_NextYear\"] = data_clean[\"Crisis_NextYear\"].astype(int)\n",
    "\n",
    "# # -------------------------\n",
    "# # 3) Prepare X, y\n",
    "# # -------------------------\n",
    "# X = data_clean[lagged_cols].fillna(data_clean[lagged_cols].median())\n",
    "# y = data_clean[\"Crisis_NextYear\"]\n",
    "# meta = data_clean[[\"Country Name\", \"Date\"]] # Remove Country Code\n",
    "\n",
    "# # -------------------------\n",
    "# # 4) Train/test split\n",
    "# # -------------------------\n",
    "# if len(np.unique(y)) == 1:\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "# else:\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# # -------------------------\n",
    "# # 5) Train RandomForest\n",
    "# # -------------------------\n",
    "# model = RandomForestClassifier(n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # -------------------------\n",
    "# # 6) Evaluate model\n",
    "# # -------------------------\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nâœ… Model Accuracy: {accuracy*100:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# # -------------------------\n",
    "# # 7) SHAP explainer\n",
    "# # -------------------------\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# sv_future = explainer.shap_values(X)[1]  # class=1\n",
    "\n",
    "# # -------------------------\n",
    "# # 8) Predict for next year\n",
    "# # -------------------------\n",
    "# last_year = data[\"Date\"].max()\n",
    "# future_rows = data[data[\"Date\"] == last_year].copy()\n",
    "# if future_rows.empty:\n",
    "#     raise ValueError(f\"No rows for Date == {last_year}\")\n",
    "\n",
    "# # Build X_future from lagged features\n",
    "# X_future = future_rows[lagged_cols].fillna(X.median())\n",
    "# future_probs = model.predict_proba(X_future)[:, 1]\n",
    "\n",
    "# # SHAP values for future\n",
    "# sv_future_rows = explainer.shap_values(X_future)\n",
    "# if isinstance(sv_future_rows, list) and len(sv_future_rows) > 1:\n",
    "#     shap_future_rows = np.array(sv_future_rows[1])\n",
    "# else:\n",
    "#     shap_future_rows = np.array(sv_future_rows)\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # 9) Prepare CSV with reason indicators\n",
    "# # -------------------------\n",
    "# results = []\n",
    "# for idx in range(len(future_rows)):\n",
    "#     country = future_rows.iloc[idx][\"Country Name\"]\n",
    "#     prob = future_probs[idx]\n",
    "\n",
    "#     # Only save countries above threshold\n",
    "#     if prob >= PROB_THRESHOLD:\n",
    "#         raw_shap = shap_future_rows[idx]\n",
    "#         # Flatten nested list if needed\n",
    "#         if isinstance(raw_shap[0], (list, np.ndarray)):\n",
    "#             instance_shap = np.array([float(x[0]) for x in raw_shap])\n",
    "#         else:\n",
    "#             instance_shap = np.array(raw_shap, dtype=float)\n",
    "\n",
    "#         # Top 3 features\n",
    "#         feat_vals = X_future.iloc[idx].values\n",
    "#         feat_names = X_future.columns.tolist()\n",
    "#         feat_pairs = list(zip(feat_names, instance_shap, feat_vals))\n",
    "#         top_feats = sorted(feat_pairs, key=lambda x: abs(x[1]), reverse=True)[:3]\n",
    "#         reason = \", \".join([f\"{f[0].replace('_lag1','')}={f[2]:.2f}\" for f in top_feats])\n",
    "\n",
    "#         results.append({\n",
    "#             \"Country Name\": country,\n",
    "#             \"Predicted_Year\": last_year+1,\n",
    "#             \"Crisis_Prob\": prob,\n",
    "#             \"Reason_Indicators\": reason\n",
    "#         })\n",
    "\n",
    "# # Save to CSV\n",
    "# if results:\n",
    "#     pd.DataFrame(results).to_csv(OUT_PRED_CSV, index=False)\n",
    "#     print(f\"\\nðŸ“‚ High-risk countries saved with reason indicators: {OUT_PRED_CSV}\")\n",
    "# else:\n",
    "#     print(\"\\nðŸŒ No countries exceeded the crisis probability threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8941,
     "status": "ok",
     "timestamp": 1760446921656,
     "user": {
      "displayName": "Swaroop Tandle",
      "userId": "04403577212311715821"
     },
     "user_tz": -330
    },
    "id": "NQedUsHazbQv",
    "outputId": "7e583922-3750-4880-f7cb-12f8b6ab6ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model Accuracy: 97.24%\n",
      "\n",
      "\n",
      "ðŸ”¥ High-Risk Countries (â‰¥80% Crisis Probability)\n",
      "\n",
      "ðŸš¨ Botswana â€” Crisis Probability: 97.00%\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.311 | Value: 23.14\n",
      "   GDP growth (annual %)                         Contribution: +0.276 | Value: -2.99\n",
      "   Domestic credit to private sector (% of GDP)  Contribution: -0.041 | Value: 32.93\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Burundi â€” Crisis Probability: 82.50%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.341 | Value: 10.80\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: -0.136 | Value: 0.90\n",
      "   Inflation, consumer prices (annual %)         Contribution: +0.123 | Value: 20.21\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Gambia, The â€” Crisis Probability: 91.00%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.516 | Value: 6.55\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: -0.114 | Value: 6.50\n",
      "   Imports of goods and services (% of GDP)      Contribution: +0.058 | Value: 37.19\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Haiti â€” Crisis Probability: 98.00%\n",
      "   GDP growth (annual %)                         Contribution: +0.198 | Value: -4.17\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.159 | Value: 3.40\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.128 | Value: 15.06\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Iraq â€” Crisis Probability: 97.50%\n",
      "   GDP growth (annual %)                         Contribution: +0.394 | Value: -1.55\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.286 | Value: 15.52\n",
      "   Exports of goods and services (% of GDP)      Contribution: -0.076 | Value: 37.51\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Libya â€” Crisis Probability: 94.00%\n",
      "   GDP growth (annual %)                         Contribution: +0.373 | Value: -0.61\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.258 | Value: 18.61\n",
      "   Exports of goods and services (% of GDP)      Contribution: -0.118 | Value: 74.83\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Namibia â€” Crisis Probability: 92.00%\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.356 | Value: 19.15\n",
      "   Imports of goods and services (% of GDP)      Contribution: +0.182 | Value: 67.98\n",
      "   GDP growth (annual %)                         Contribution: -0.090 | Value: 3.71\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Nepal â€” Crisis Probability: 91.00%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.405 | Value: 7.62\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.137 | Value: 10.71\n",
      "   GDP growth (annual %)                         Contribution: -0.083 | Value: 3.67\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Sudan â€” Crisis Probability: 96.50%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.201 | Value: 1.19\n",
      "   GDP growth (annual %)                         Contribution: +0.191 | Value: -13.49\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.158 | Value: 10.90\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Syrian Arab Republic â€” Crisis Probability: 96.00%\n",
      "   GDP growth (annual %)                         Contribution: +0.313 | Value: -3.10\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.285 | Value: 12.96\n",
      "   Domestic credit to private sector (% of GDP)  Contribution: -0.039 | Value: 65.70\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ United States â€” Crisis Probability: 99.00%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.330 | Value: 10.90\n",
      "   Domestic credit to private sector (% of GDP)  Contribution: +0.204 | Value: 197.90\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: -0.089 | Value: 4.11\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ Yemen, Rep. â€” Crisis Probability: 92.00%\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: +0.300 | Value: 17.09\n",
      "   GDP growth (annual %)                         Contribution: +0.286 | Value: -3.80\n",
      "   Imports of goods and services (% of GDP)      Contribution: -0.059 | Value: 31.80\n",
      "----------------------------------------------------------------------\n",
      "ðŸš¨ North America â€” Crisis Probability: 99.50%\n",
      "   Exports of goods and services (% of GDP)      Contribution: +0.318 | Value: 12.45\n",
      "   Domestic credit to private sector (% of GDP)  Contribution: +0.210 | Value: 197.90\n",
      "   Unemployment, total (% of total labor force) (modeled ILO estimate) Contribution: -0.089 | Value: 4.38\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "\n",
    "# ===============================\n",
    "# 1. Load dataset\n",
    "# ===============================\n",
    "data = pd.read_csv(r\"C:\\Users\\Eeksha\\Desktop\\GSECD-final-project1.0\\version2\\civic-data-forge-main\\supabase\\models\\economic_dataset_crisis_label.csv\")\n",
    "data[\"Date\"] = data[\"Date\"].astype(int)\n",
    "\n",
    "\n",
    "# Features and target\n",
    "X = data.drop(columns=[\"Crisis_Flag\", \"Country Name\", \"Date\", \"Country Code\", \"Crisis_Level\", \"Crisis_Score\"], errors=\"ignore\")\n",
    "y = data[\"Crisis_Flag\"]\n",
    "country_year = data[[\"Country Name\", \"Date\"]]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test, cy_train, cy_test = train_test_split(\n",
    "    X, y, country_year, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 2. Train Ensemble Model\n",
    "# ===============================\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nâœ… Model Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Predict for Next Year (2025)\n",
    "# ===============================\n",
    "latest_year = data[\"Date\"].max()\n",
    "future_data = data[data[\"Date\"] == latest_year].copy()\n",
    "future_data[\"Date\"] = latest_year + 1  # simulate 2025\n",
    "\n",
    "X_future = future_data.drop(columns=[\"Crisis_Flag\", \"Country Name\", \"Date\", \"Country Code\", \"Crisis_Level\", \"Crisis_Score\"], errors=\"ignore\")\n",
    "future_probs = model.predict_proba(X_future)[:, 1]\n",
    "\n",
    "# Add predictions\n",
    "future_data[\"Crisis_Probability\"] = future_probs\n",
    "\n",
    "# ===============================\n",
    "# 4. Identify High-Risk Countries\n",
    "# ===============================\n",
    "threshold = 0.80  # show countries with â‰¥80% crisis probability\n",
    "high_risk = future_data[future_data[\"Crisis_Probability\"] >= threshold]\n",
    "\n",
    "if high_risk.empty:\n",
    "    print(\"ðŸŒ No countries exceed the crisis probability threshold.\")\n",
    "else:\n",
    "    # SHAP explanation\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_future) # This will return a list of arrays\n",
    "\n",
    "    # Select the SHAP values for the positive class (class 1)\n",
    "    if isinstance(shap_values, list) and len(shap_values) > 1:\n",
    "        shap_values_positive = shap_values[1]\n",
    "    else:\n",
    "        shap_values_positive = shap_values\n",
    "\n",
    "    # Flatten nested list if needed\n",
    "    # Check if the shape is (n_instances, n_features, n_classes)\n",
    "    if len(shap_values_positive.shape) == 3:\n",
    "        # Select the SHAP values for the positive class (index 1) for each feature\n",
    "        shap_values_positive_flat = shap_values_positive[:, :, 1]\n",
    "    else:\n",
    "        # Assume it's already in the shape (n_instances, n_features)\n",
    "        shap_values_positive_flat = shap_values_positive\n",
    "\n",
    "\n",
    "    # Convert shap_values to a DataFrame to align with X_future and then high_risk\n",
    "    shap_df = pd.DataFrame(shap_values_positive_flat, columns=X_future.columns, index=X_future.index)\n",
    "    high_risk_shap = shap_df.loc[high_risk.index]\n",
    "\n",
    "\n",
    "    print(\"\\nðŸ”¥ High-Risk Countries (â‰¥80% Crisis Probability)\\n\")\n",
    "    for idx in high_risk.index:\n",
    "        country = high_risk.loc[idx, \"Country Name\"]\n",
    "        prob = high_risk.loc[idx, \"Crisis_Probability\"] * 100\n",
    "\n",
    "        # Get feature importance for this country\n",
    "        instance_shap = high_risk_shap.loc[idx].values\n",
    "        feature_contribs = sorted(\n",
    "            zip(X_future.columns, instance_shap, X_future.loc[idx]),\n",
    "            key=lambda x: abs(x[1]),\n",
    "            reverse=True\n",
    "        )[:3]\n",
    "\n",
    "        print(f\"ðŸš¨ {country} â€” Crisis Probability: {prob:.2f}%\")\n",
    "        for feat, contrib, val in feature_contribs:\n",
    "            print(f\"   {feat:<45} Contribution: {contrib:+.3f} | Value: {val:.2f}\")\n",
    "        print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuLaul89FaNy"
   },
   "source": [
    "final_dataset_GSECD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['economic_crisis_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'economic_crisis_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved economic_model.pkl with feature order: ['Domestic credit to private sector (% of GDP)', 'Exports of goods and services (% of GDP)', 'GDP growth (annual %)', 'GDP per capita (current US$)', 'Gross fixed capital formation (% of GDP)', 'Imports of goods and services (% of GDP)', 'Inflation, consumer prices (annual %)', 'Unemployment, total (% of total labor force) (modeled ILO estimate)']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model and feature order\n",
    "model_data = {\n",
    "    \"model\": model,\n",
    "    \"feature_order\": list(X.columns)\n",
    "}\n",
    "\n",
    "with open(\"economic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"âœ… Saved economic_model.pkl with feature order:\", model_data[\"feature_order\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ðŸ“Š FINAL MODEL EVALUATION METRICS\n",
      "==============================\n",
      "âœ… Accuracy:  0.9724\n",
      "âœ… Precision: 0.9653\n",
      "âœ… Recall:    0.8797\n",
      "âœ… F1 Score:  0.9205\n",
      "âœ… ROC-AUC:   0.995275387569336\n",
      "\n",
      "ðŸ” Confusion Matrix:\n",
      "[[707   5]\n",
      " [ 19 139]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       712\n",
      "           1       0.97      0.88      0.92       158\n",
      "\n",
      "    accuracy                           0.97       870\n",
      "   macro avg       0.97      0.94      0.95       870\n",
      "weighted avg       0.97      0.97      0.97       870\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"ðŸ“Š FINAL MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Try to get ROC-AUC if probabilities are available\n",
    "try:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        roc_auc = \"N/A (No probability scores)\"\n",
    "except:\n",
    "    roc_auc = \"N/A\"\n",
    "\n",
    "# Print results\n",
    "print(f\"âœ… Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall:    {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score:  {f1:.4f}\")\n",
    "print(f\"âœ… ROC-AUC:   {roc_auc}\")\n",
    "print(\"\\nðŸ” Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNd9vmBPs2+ji5bGgtaBjw7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
